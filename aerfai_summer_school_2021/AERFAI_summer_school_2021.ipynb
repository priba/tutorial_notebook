{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AERFAI_summer_school_2021.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priba/tutorial_notebook/blob/main/aerfai_summer_school_2021/AERFAI_summer_school_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd0__tX7I7Ji"
      },
      "source": [
        "# Graph Neural Networks for Pattern Recognition\n",
        "## AERFAI - Online Summer School on Pattern Recognition and Machine Learning\n",
        "\n",
        "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/priba/tutorial_notebook/blob/main/aerfai_summer_school_2021/AERFAI_summer_school_2021.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "Graph Neural Networks (GNNs) have recently gained increasing popularity due to its novel uses in different domains such as Computer Vision, Natural Language Processing, Chemistry, Social Networks or Knowledge Graphs. In this tutorial, we will cover the basic usage of GNN for the problem of node or graph classification. In particular, we will build a GNN from scratch to understand the explained notation and concepts. Later, we will use the provided implementations of several GNN to check the difference in performance.\n",
        "\n",
        "We will use [PyTorch](https://pytorch.org/) as our Deep Learning framework and the [Deep Graph Library (DGL)](https://www.dgl.ai/) as our GNN library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o8d3mqWpYvb"
      },
      "source": [
        "## Prepare environment\n",
        "\n",
        "Let's start preparing the environment we will use all over the session. First we will change the runtime to work on GPUs and later, we will install all the required libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCXuzl_kNMfe"
      },
      "source": [
        "### Change runtime of notebook to GPU\n",
        "\n",
        "\n",
        "```\n",
        "  Select Runtime -> Change Runtime type -> select runtime python 3 and hardward accelerator GPU\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wss6c9JtNP9g"
      },
      "source": [
        "### Install requirements\n",
        "\n",
        "The basic libraries that will be used are:\n",
        "\n",
        "*   [Network](https://networkx.github.io/)\n",
        "*   [Pytorch](https://pytorch.org/)\n",
        "*   [DGL](https://www.dgl.ai/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-nm23YSArxH"
      },
      "source": [
        "!pip3 install torch\n",
        "!pip3 install dgl-cu110"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFoB_xLWjdTc"
      },
      "source": [
        "## Get the Data\n",
        "\n",
        "The DGL provides several classes for reading graphs!\n",
        "\n",
        "Check:  https://docs.dgl.ai/api/python/dgl.data.html\n",
        "\n",
        "In the following sections we will see how we can load their data, and how it is represented. Finally, we will review how to use a custom dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_bzijh9jtBe"
      },
      "source": [
        "### Mini Graph Classification Dataset (MiniGCDataset)\n",
        "\n",
        "This dataset is synthetic and we should define de number and size of the graphs for each set (train, validation and test).\n",
        "\n",
        "*   class 0 : cycle graph\n",
        "*   class 1 : star graph\n",
        "*   class 2 : wheel graph\n",
        "*   class 3 : lollipop graph\n",
        "*   class 4 : hypercube graph\n",
        "*   class 5 : grid graph\n",
        "*   class 6 : clique graph\n",
        "*   class 7 : circular ladder graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSiiF2-68YKf"
      },
      "source": [
        "import dgl\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "dgl.backend = 'pytorch'\n",
        "\n",
        "dataset = dgl.data.MiniGCDataset(100, 23, 50)\n",
        "g, label = dataset[50]\n",
        "\n",
        "# Data structure\n",
        "print(g)\n",
        "\n",
        "# Node Information (empty in the case of MiniGCDataset)\n",
        "print(g.ndata)\n",
        "\n",
        "# Edge Information (empty in the case of simple graphs such as Letters)\n",
        "print(g.edata)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots()\n",
        "G = g.to_networkx()\n",
        "nx.draw(G, ax=ax, pos=nx.circular_layout(G))\n",
        "ax.set_title('Class: {:d}'.format(label))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1kSqi2Kkdsh"
      },
      "source": [
        "### Graph kernel dataset\n",
        "\n",
        "It allows loading the data from: ENZYMES, DD, COLLAB and MUTAG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf6ZyoCIkAn-"
      },
      "source": [
        "dataset = dgl.data.TUDataset('MUTAG')\n",
        "g, label = dataset[50]\n",
        "\n",
        "# Data structure\n",
        "print(g)\n",
        "\n",
        "# Node Information (empty in the case of MiniGCDataset)\n",
        "print(g.ndata)\n",
        "\n",
        "# Edge Information (empty in the case of simple graphs such as Letters)\n",
        "print(g.edata)\n",
        "\n",
        "# Label\n",
        "print(label)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots()\n",
        "G = g.to_networkx()\n",
        "nx.draw(G, ax=ax, pos=nx.circular_layout(G))\n",
        "ax.set_title('Class: {:d}'.format(label.item()))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQodjJkQlB4-"
      },
      "source": [
        "### Cora Graph Dataset\n",
        "\n",
        "Is a citation network graph.\n",
        "\n",
        "Nodes mean paper and edges mean citation relationships. Each node has a predefined feature with 1433 dimensions. The dataset is designed for the node classification task. The task is to predict the category of certain paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AORY5wwrlCHW"
      },
      "source": [
        "dataset = dgl.data.CoraGraphDataset()\n",
        "g = dataset[0]\n",
        "\n",
        "# Data structure\n",
        "print(g)\n",
        "\n",
        "# Node Information (empty in the case of MiniGCDataset)\n",
        "print(g.ndata)\n",
        "\n",
        "# Edge Information (empty in the case of simple graphs such as Letters)\n",
        "print(g.edata)\n",
        "\n",
        "# Node classification dataset divides the nodes into training, validation and test sets\n",
        "print(g.ndata['train_mask'])\n",
        "print(g.ndata['val_mask'])\n",
        "print(g.ndata['test_mask'])\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots()\n",
        "G = g.to_networkx()\n",
        "nx.draw(G, ax=ax, pos=nx.circular_layout(G))\n",
        "ax.set_title('Class: {:d}'.format(label.item()))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjJf_8AMm-Yv"
      },
      "source": [
        "### To Define a custom Dataset\n",
        "Pytorch provides an abstract class representig a dataset, ```torch.utils.data.Dataset```. We need to override two methods:\n",
        "\n",
        "*   ```__len__``` so that ```len(dataset)``` returns the size of the dataset.\n",
        "*   ```__getitem__``` to support the indexing such that ```dataset[i]``` can be used to get i-th sample\n",
        "\n",
        "Let us see how we can deal with a custom dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYjoNiuaNf6D"
      },
      "source": [
        "#### Letter Database\n",
        "\n",
        "Graphs that represent distorted letter drawings. They consider the 15 capital letters of the Roman alphabet that consist of straight lines only (A, E, F, H, I, K, L, M, N, T, V, W, X, Y, Z). Each node is labeled with a two-dimensional attribute giving its position relative to a reference coordinate system. Edges are unlabeled. The graph database consists of a training set, a validation set, and a test set of size 750 each. Also, three levels of distortions are provided.\n",
        "\n",
        "This dataset is part of [IAM Graph Database Repository](http://www.fki.inf.unibe.ch/databases/iam-graph-database) and it is also linked in the [IAPR TC15 resources](https://iapr-tc15.greyc.fr/links.html).\n",
        "\n",
        "It can be considered as a **TOY EXAMPLE** for graph classification.\n",
        "\n",
        "> Riesen, K. and Bunke, H.: [IAM Graph Database Repository for Graph Based Pattern Recognition and Machine Learning.](https://link.springer.com/chapter/10.1007/978-3-540-89689-0_33) In: da Vitora Lobo, N. et al. (Eds.), SSPR&SPR 2008, LNCS, vol. 5342, pp. 287-297, 2008.\n",
        "\n",
        "Lets get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNu3EKNANSjy"
      },
      "source": [
        "!wget https://iapr-tc15.greyc.fr/IAM/Letter.zip\n",
        "!unzip Letter.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGLvIa1yNUwg"
      },
      "source": [
        "#### Prepare data reader\n",
        "\n",
        "IAM graphs are provided as a GXL file:\n",
        "\n",
        "\n",
        "```\n",
        "<gxl>\n",
        "  <graph id=\"GRAPH_ID\" edgeids=\"false\" edgemode=\"undirected\">\n",
        "    <node id=\"_0\">\n",
        "      <attr name=\"x\">\n",
        "        <float>0.812867</float>\n",
        "      </attr>\n",
        "      <attr name=\"y\">\n",
        "        <float>0.630453</float>\n",
        "      </attr>\n",
        "    </node>\n",
        "    ...\n",
        "    <node id=\"_N\">\n",
        "      ...\n",
        "    </node>\n",
        "    <edge from=\"_0\" to=\"_1\"/>\n",
        "    ...\n",
        "    <edge from=\"_M\" to=\"_N\"/>\n",
        "  </graph>\n",
        "</gxl>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qldLZ1kcNktl"
      },
      "source": [
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "import networkx as nx\n",
        "\n",
        "\n",
        "def read_letters(file):\n",
        "  \"\"\"Parse GXL file and returns a networkx graph\n",
        "  \"\"\"\n",
        "  \n",
        "  tree_gxl = ET.parse(file)\n",
        "  root_gxl = tree_gxl.getroot()\n",
        "  node_label = {}\n",
        "  node_id = []\n",
        "  \n",
        "  # Parse nodes\n",
        "  for i, node in enumerate(root_gxl.iter('node')):\n",
        "    node_id += [node.get('id')]\n",
        "    for attr in node.iter('attr'):\n",
        "      if (attr.get('name') == 'x'):\n",
        "        x = float(attr.find('float').text)\n",
        "      elif (attr.get('name') == 'y'):\n",
        "        y = float(attr.find('float').text)\n",
        "    node_label[i] = [x, y]\n",
        "\n",
        "  node_id = np.array(node_id)\n",
        "\n",
        "  # Create adjacency matrix\n",
        "  am = np.zeros((len(node_id), len(node_id)))\n",
        "  for edge in root_gxl.iter('edge'):\n",
        "    s = np.where(node_id==edge.get('from'))[0][0]\n",
        "    t = np.where(node_id==edge.get('to'))[0][0]\n",
        "\n",
        "    # Undirected Graph\n",
        "    am[s,t] = 1\n",
        "    am[t,s] = 1\n",
        "\n",
        "  # Create the networkx graph\n",
        "  G = nx.from_numpy_matrix(am)\n",
        "  nx.set_node_attributes(G, node_label, 'position')\n",
        "  \n",
        "  return G"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp4TwyTeNuRo"
      },
      "source": [
        "#### Dataset Division\n",
        "\n",
        "The dataset is divided by means of CXL files in *train*, *validation* and *test* with the correspondance filename and class:\n",
        "\n",
        "\n",
        "```\n",
        "<GraphCollection>\n",
        "  <fingerprints base=\"/scratch/mneuhaus/progs/letter-database/automatic/0.1\" classmodel=\"henry5\" count=\"750\">\n",
        "    <print file=\"AP1_0100.gxl\" class=\"A\"/>\n",
        "    ...\n",
        "    <print file=\"ZP1_0149.gxl\" class=\"Z\"/>\n",
        "  </fingerprints>\n",
        "</GraphCollection>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvLX7_XtNvbD"
      },
      "source": [
        "def getFileList(file_path):\n",
        "  \"\"\"Parse CXL file and returns the corresponding file list and class\n",
        "  \"\"\"\n",
        "  \n",
        "  elements, classes = [], []\n",
        "  tree = ET.parse(file_path)\n",
        "  root = tree.getroot()\n",
        "  \n",
        "  for child in root:\n",
        "    for sec_child in child:\n",
        "      if sec_child.tag == 'print':\n",
        "        elements += [sec_child.attrib['file']]\n",
        "        classes += sec_child.attrib['class']\n",
        "        \n",
        "  return elements, classes"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LK-34NcNzM9"
      },
      "source": [
        "#### Define Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Bxob9dEN6XD"
      },
      "source": [
        "import torch.utils.data as data\n",
        "import os\n",
        "\n",
        "class Letters(data.Dataset):\n",
        "  \"\"\"Letter Dataset\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, root_path, file_list):\n",
        "    self.root = root_path\n",
        "    self.file_list = file_list\n",
        "    \n",
        "    # List of files and corresponding labels\n",
        "    self.graphs, self.labels = getFileList(os.path.join(self.root, self.file_list))\n",
        "    \n",
        "    # Labels to numeric value\n",
        "    self.unique_labels = np.unique(self.labels)\n",
        "    self.num_classes = len(self.unique_labels)\n",
        "    \n",
        "    self.labels = [np.where(target == self.unique_labels)[0][0] \n",
        "                   for target in self.labels]\n",
        "    \n",
        "    \n",
        "  def __getitem__(self, index):\n",
        "    # Read the graph and label\n",
        "    G = read_letters(os.path.join(self.root, self.graphs[index]))\n",
        "    target = self.labels[index]\n",
        "    \n",
        "    # Convert to DGL format\n",
        "    g = dgl.from_networkx(G, node_attrs=['position'])\n",
        "        \n",
        "    return g, target\n",
        "  \n",
        "  def label2class(self, label):\n",
        "    # Converts the numeric label to the corresponding string\n",
        "    return self.unique_labels[label]\n",
        "  \n",
        "  def __len__(self):\n",
        "    # Subset length\n",
        "    return len(self.labels)\n",
        "\n",
        "# Define the corresponding subsets for train, validation and test.\n",
        "dataset = Letters(os.path.join('Letter', 'LOW'), 'train.cxl')"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tftRBszSStAy"
      },
      "source": [
        "Let's see what we have constructed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfggzCFKOH2Z"
      },
      "source": [
        "g, label = dataset[0]\n",
        "\n",
        "# Data structure\n",
        "print(g)\n",
        "\n",
        "# Node Information (empty in the case of MiniGCDataset)\n",
        "print(g.ndata)\n",
        "\n",
        "# Edge Information (empty in the case of simple graphs such as Letters)\n",
        "print(g.edata)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots()\n",
        "G = g.to_networkx(node_attrs=['position'])\n",
        "position = dict(G.nodes(data='position'))\n",
        "\n",
        "position = {k: v.numpy() for k, v in position.items()}\n",
        "nx.draw(G, ax=ax, pos=position)\n",
        "ax.set_title('Class: {:d}'.format(label.item()))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTP_A5pOFOF9"
      },
      "source": [
        "## GNN tasks\n",
        "\n",
        "<center width=\"100%\" style=\"padding:10px\"><img src=\"https://github.com/priba/tutorial_notebook/blob/main/aerfai_summer_school_2021/gnn_task.png?raw=1\" width=\"800px\"></center>\n",
        "\n",
        "In this tutorial we will cover:\n",
        "\n",
        "\n",
        "*   Graph Classification\n",
        "*   Node classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FWDCyjjPXl9"
      },
      "source": [
        "## Prepare DataLoader\n",
        "\n",
        "In this stage, we will continue our tutorial with the ```MiniGCDataset``` dataset.\n",
        "\n",
        "```torch.utils.data.DataLoader``` is an iterator which provides:\n",
        "\n",
        "\n",
        "*   Data batching\n",
        "*   Shuffling the data\n",
        "*   Parallel data loading\n",
        "\n",
        "In our specific case, we need to deal with graphs of many sizes. Hence, we define a new collate function makin guse of the method ```dgl.batch```.\n",
        "\n",
        "In this example we will work with the MiniGCDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea8B1D8z-TlB"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate(samples):\n",
        "    # The input `samples` is a list of pairs\n",
        "    #  (graph, label).\n",
        "    graphs, labels = map(list, zip(*samples))\n",
        "    batched_graph = dgl.batch(graphs)\n",
        "    return batched_graph, torch.tensor(labels)\n",
        "\n",
        "# Define the corresponding subsets for train, validation and test.\n",
        "trainset = dgl.data.MiniGCDataset(1000, 23, 50)\n",
        "validset = dgl.data.MiniGCDataset(100, 23, 100)\n",
        "testset = dgl.data.MiniGCDataset(500, 23, 100)\n",
        "\n",
        "# Define the three dataloaders. Train data will be shuffled at each epoch\n",
        "train_loader = DataLoader(trainset, batch_size=32, shuffle=True,\n",
        "                         collate_fn=collate)\n",
        "valid_loader = DataLoader(validset, batch_size=32, collate_fn=collate)\n",
        "test_loader = DataLoader(testset, batch_size=32, collate_fn=collate)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cRRfYBqnHJA"
      },
      "source": [
        "## Define GNN model\n",
        "\n",
        "We have two options.\n",
        "\n",
        "*   To define our own GNN layers (see next section)\n",
        "*   To use predefined GNN layers (skip to NN Modules)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-9eZEvJPb_r"
      },
      "source": [
        "### Define our own Model\n",
        "\n",
        "To define a Graph Convolution, three functions have to be defined:\n",
        "\n",
        "*   Message: Decide which information is sent by a node\n",
        "*   Reduce: Aggregate the messages.\n",
        "*   NodeApply: Update the node features that are recieved from the reduce function\n",
        "\n",
        "<center width=\"100%\" style=\"padding:10px\"><img src=\"https://github.com/priba/tutorial_notebook/blob/main/aerfai_summer_school_2021/gnn_layer.png?raw=1\" width=\"800px\"></center>\n",
        "\n",
        "Let us see how can we define this functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02lS2sM6Pa_e"
      },
      "source": [
        "import dgl.function as fn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Sends a message of node feature h.\n",
        "msg = fn.copy_src(src='h', out='m')\n",
        "def message_func(edges):\n",
        "    return {'m': edges.src['h']}\n",
        "\n",
        "def reduce(nodes):\n",
        "  \"\"\"Take an average over all neighbor node features hu and use it to\n",
        "  overwrite the original node feature.\"\"\"\n",
        "  accum = torch.sum(nodes.mailbox['m'], 1)\n",
        "  return {'m': accum}\n",
        "\n",
        "class NodeApplyModule(nn.Module):\n",
        "  \"\"\"Update the node feature hv with ReLU(Whv+b).\"\"\"\n",
        "  def __init__(self, in_feats, out_feats, activation):\n",
        "    super(NodeApplyModule, self).__init__()\n",
        "    self.linear = nn.Linear(in_feats, out_feats)\n",
        "    self.activation = activation\n",
        "\n",
        "  def forward(self, node):\n",
        "    h = torch.cat([node.data['h'], node.data['m']],1)\n",
        "    h = self.linear(h)\n",
        "    h = self.activation(h)\n",
        "    return {'h' : h}  \n",
        "  \n",
        "class GNN(nn.Module):\n",
        "  \"\"\"Define a GNN layer\"\"\"\n",
        "  def __init__(self, in_feats, out_feats, activation):\n",
        "    super(GNN, self).__init__()\n",
        "    self.apply_mod = NodeApplyModule(2*in_feats, out_feats, activation)\n",
        "    \n",
        "  def forward(self, g, feature):\n",
        "    # Initialize the node features with h.\n",
        "    g.ndata['h'] = feature\n",
        "    g.update_all(msg, reduce)\n",
        "    g.apply_nodes(func=self.apply_mod)\n",
        "    return g.ndata.pop('h')\n",
        "  \n",
        "class Net(nn.Module):\n",
        "  def __init__(self, in_dim, hidden_dim, n_classes):\n",
        "    super(Net, self).__init__()\n",
        "    self.layers = nn.ModuleList([\n",
        "        GNN(in_dim, hidden_dim, F.relu),\n",
        "        GNN(hidden_dim, hidden_dim, F.relu)])\n",
        "    self.classify = nn.Linear(hidden_dim, n_classes)\n",
        "    \n",
        "  def forward(self, g):\n",
        "    # For undirected graphs, in_degree is the same as\n",
        "    # out_degree.\n",
        "    h = g.in_degrees().view(-1, 1).float()\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "      h = h.cuda() \n",
        "    for conv in self.layers:\n",
        "      h = conv(g, h)\n",
        "    g.ndata['h'] = h\n",
        "    hg = dgl.mean_nodes(g, 'h')\n",
        "    return self.classify(hg)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exF4za6k8Yj_"
      },
      "source": [
        "#### Training example for graph classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zSKxrln8WBH"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "model = Net(1, 256, trainset.num_classes)\n",
        "if torch.cuda.is_available():\n",
        "  model = model.cuda()\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "model.train()\n",
        "\n",
        "epoch_losses = []\n",
        "for epoch in range(80):\n",
        "  epoch_loss = 0\n",
        "  for iter, (bg, label) in enumerate(train_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      label = label.cuda()\n",
        "      bg = bg.to(label.device)\n",
        "    prediction = model(bg)\n",
        "    loss = loss_func(prediction, label)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.detach().item()\n",
        "  epoch_loss /= (iter + 1)\n",
        "  print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n",
        "  epoch_losses.append(epoch_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v48cp3EH8g1S"
      },
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBkG0WAy8i1i"
      },
      "source": [
        "def accuracy(output, target):\n",
        "  \"\"\"Accuacy given a logit vector output and a target class\n",
        "  \"\"\"\n",
        "  _, pred = output.topk(1)\n",
        "  pred = pred.squeeze()\n",
        "  correct = pred == target\n",
        "  correct = correct.float()\n",
        "  return correct.sum() * 100.0 / correct.shape[0]\n",
        "\n",
        "\n",
        "model.eval()\n",
        "acc = 0\n",
        "with torch.no_grad():\n",
        "  for iter, (bg, label) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "        label = label.cuda()\n",
        "        bg = bg.to(label.device)\n",
        "    prediction = model(bg)\n",
        "    acc += accuracy(prediction, label) * label.shape[0]\n",
        "acc = acc/len(testset)\n",
        "\n",
        "print('Test accuracy {:.4f}'.format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOrJRh0L8qVB"
      },
      "source": [
        "#### Let us see some qualitative results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFwafB1i8pyu"
      },
      "source": [
        "from random import randrange\n",
        "import matplotlib.pyplot as plt\n",
        "for i in range(10):\n",
        "  index = randrange(len(testset))\n",
        "  g, label = testset[index]\n",
        "  pred = model(g.to('cuda'))\n",
        "  _, pred = pred.topk(1)\n",
        "  G = g.to_networkx()\n",
        "  plt.figure(i)\n",
        "  nx.draw(G, pos=nx.circular_layout(G), arrows=False)\n",
        "  plt.show()\n",
        "  print('Label {}; Prediction {}'.format(label, pred.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXZ20gTpY3lK"
      },
      "source": [
        "### NN Modules\n",
        "\n",
        "Several GNN layers are provided by default as [NN Modules](https://docs.dgl.ai/api/python/nn.pytorch.html#graphconv).\n",
        "\n",
        "**Choose the network to use, either with NN Modules or defining it as it has been shown before.**\n",
        "\n",
        "Lets see one example with the classical GraphConv\n",
        "\n",
        "$$\n",
        "h_i^{(l+1)} = \\sigma(b^{(l)} + \\sum_{j\\in\\mathcal{N}(i)}\\frac{1}{c_{ji}}h_j^{(l)}W^{(l)})\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc5FnY8gY1PJ"
      },
      "source": [
        "from dgl.nn.pytorch import GraphConv\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self, in_dim, hidden_dim, n_classes):\n",
        "    super(Net, self).__init__()\n",
        "    self.layers = nn.ModuleList([\n",
        "        GraphConv(in_dim, hidden_dim, activation=F.relu),\n",
        "        GraphConv(hidden_dim, hidden_dim, activation=F.relu)])\n",
        "    self.classify = nn.Linear(hidden_dim, n_classes)\n",
        "    \n",
        "  def forward(self, g):\n",
        "    # Use node degree as the initial node feature. For undirected graphs, the in-degree\n",
        "    # is the same as the out_degree.\n",
        "    h = g.in_degrees().view(-1, 1).float()\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "      h = h.cuda() \n",
        "      \n",
        "    for conv in self.layers:\n",
        "      h = conv(g, h)\n",
        "      \n",
        "    g.ndata['h'] = h\n",
        "    hg = dgl.mean_nodes(g, 'h')\n",
        "    return self.classify(hg)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pPWJfxYUyXg"
      },
      "source": [
        "#### Training example for graph classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inHmWtgwUxy-"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "model = Net(1, 256, trainset.num_classes)\n",
        "if torch.cuda.is_available():\n",
        "  model = model.cuda()\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "model.train()\n",
        "\n",
        "epoch_losses = []\n",
        "for epoch in range(80):\n",
        "  epoch_loss = 0\n",
        "  for iter, (bg, label) in enumerate(train_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      label = label.cuda()\n",
        "      bg = bg.to(label.device)\n",
        "    prediction = model(bg)\n",
        "    loss = loss_func(prediction, label)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.detach().item()\n",
        "  epoch_loss /= (iter + 1)\n",
        "  print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n",
        "  epoch_losses.append(epoch_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkxZZXdT9Kq_"
      },
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-peNt4q9861W"
      },
      "source": [
        "def accuracy(output, target):\n",
        "  \"\"\"Accuacy given a logit vector output and a target class\n",
        "  \"\"\"\n",
        "  _, pred = output.topk(1)\n",
        "  pred = pred.squeeze()\n",
        "  correct = pred == target\n",
        "  correct = correct.float()\n",
        "  return correct.sum() * 100.0 / correct.shape[0]\n",
        "\n",
        "\n",
        "model.eval()\n",
        "acc = 0\n",
        "with torch.no_grad():\n",
        "  for iter, (bg, label) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "        label = label.cuda()\n",
        "        bg = bg.to(label.device)\n",
        "    prediction = model(bg)\n",
        "    acc += accuracy(prediction, label) * label.shape[0]\n",
        "acc = acc/len(testset)\n",
        "\n",
        "print('Test accuracy {:.4f}'.format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-R1ZDmqkdN0"
      },
      "source": [
        "#### Plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_Lkp4sqeg8T"
      },
      "source": [
        "from random import randrange\n",
        "import matplotlib.pyplot as plt\n",
        "for i in range(10):\n",
        "  index = randrange(len(testset))\n",
        "  g, label = testset[index]\n",
        "  pred = model(g.to('cuda'))\n",
        "  _, pred = pred.topk(1)\n",
        "  G = g.to_networkx()\n",
        "  plt.figure(i)\n",
        "  nx.draw(G, pos=nx.circular_layout(G), arrows=False)\n",
        "  plt.show()\n",
        "  print('Label {}; Prediction {}'.format(label, pred.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh80-RWqEnC_"
      },
      "source": [
        "## Node classification example\n",
        "\n",
        "We will study this problem with the Cora Graph Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHSC25SmJfEz"
      },
      "source": [
        "### Define the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UjjHiR-Kmen"
      },
      "source": [
        "dataset = dgl.data.CiteseerGraphDataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSGmfqqzCHsi"
      },
      "source": [
        "dataset[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8A4oN4812qv"
      },
      "source": [
        "### Define the model\n",
        "\n",
        "Note that now, we do not require a global pooling (Readout function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GhTV2Ag11Mi"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, in_dim, hidden_dim, n_classes):\n",
        "    super(Net, self).__init__()\n",
        "    self.layers = nn.ModuleList([\n",
        "        GraphConv(in_dim, hidden_dim, activation=F.relu),\n",
        "        GraphConv(hidden_dim, hidden_dim, activation=F.relu)])\n",
        "    self.classify = nn.Linear(hidden_dim, n_classes)\n",
        "    self.drop = nn.Dropout(p=0.2)\n",
        "    \n",
        "  def forward(self, g, feat):\n",
        "    \n",
        "    h = feat\n",
        "    if torch.cuda.is_available():\n",
        "      h = h.cuda() \n",
        "      \n",
        "    for conv in self.layers:\n",
        "      h = conv(g, h)\n",
        "      h = self.drop(h)\n",
        "    \n",
        "    h = self.classify(h)\n",
        "    g.ndata['h'] = h\n",
        "    return h"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54nIt0IJ2Po0"
      },
      "source": [
        "### Training example for node classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhcBDUFL2P6_"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "model = Net(dataset[0].ndata['feat'].shape[1], 64, 6)\n",
        "if torch.cuda.is_available():\n",
        "  model = model.cuda()\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "model.train()\n",
        "\n",
        "epoch_losses = []\n",
        "\n",
        "graph = dataset[0] # We only have one graph\n",
        "if torch.cuda.is_available():\n",
        "  graph = graph.to('cuda')\n",
        "\n",
        "train_mask = graph.ndata['train_mask']\n",
        "val_mask = graph.ndata['val_mask']\n",
        "test_mask = graph.ndata['test_mask']\n",
        "\n",
        "for epoch in range(80):\n",
        "  model.train()\n",
        "\n",
        "  prediction = model(graph, graph.ndata['feat'])\n",
        "  prediction = prediction.squeeze()\n",
        "  loss = loss_func(prediction[train_mask], graph.ndata['label'][train_mask])\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  \n",
        "  prediction = model(graph, graph.ndata['feat'])\n",
        "  acc = accuracy(prediction[val_mask], graph.ndata['label'][val_mask])\n",
        "  \n",
        "  epoch_loss = loss.detach().item()\n",
        "  print('Epoch {}, loss {:.4f}; Validation Accuracy {:.4f}'.format(epoch, epoch_loss, acc))\n",
        "  epoch_losses.append(epoch_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlYK6G9nGY82"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNr9itRQGcMF"
      },
      "source": [
        "model.eval()\n",
        "  \n",
        "prediction = model(graph, graph.ndata['feat'])\n",
        "prediction = prediction.squeeze()\n",
        "acc = accuracy(prediction[test_mask], graph.ndata['label'][test_mask])\n",
        "\n",
        "print('Test Accuracy {:.4f}'.format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1MLL1wtEX3n"
      },
      "source": [
        "# Other resources\n",
        "\n",
        "\n",
        "\n",
        "*   https://nn.labml.ai/graphs/gat/index.html\n",
        "\n"
      ]
    }
  ]
}